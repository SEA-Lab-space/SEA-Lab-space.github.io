@inproceedings{xu_earbuddy_2020,
 abstract = {Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classiﬁcation. Our optimized classiﬁer achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability.},
 address = {Honolulu HI USA},
 author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, Wenjia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K},
 booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems},
 file = {Xu et al. - EarBuddy Enabling On-Face Interaction via Wireles.pdf:/Users/orsonxu/Zotero/storage/6BXBWNYE/Xu et al. - EarBuddy Enabling On-Face Interaction via Wireles.pdf:application/pdf},
 language = {en},
 pages = {14},
 publisher = {ACM},
 title = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds},
 year = {2020}
}
