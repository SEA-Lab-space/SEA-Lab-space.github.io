@inproceedings{chen_understanding_2021,
 abstract = {As wearable devices move toward the face (i.e. smart earbuds, glasses), there is an increasing need to facilitate intuitive interactions with these devices. Current sensing techniques can already detect many mouth-based gestures; however, users’ preferences of these gestures are not fully understood. In this paper, we investigate the design space and usability of mouth-based microgestures. We first conducted brainstorming sessions (N=16) and compiled an extensive set of 86 user-defined gestures. Then, with an online survey (N=50), we assessed the physical and mental demand of our gesture set and identified a subset of 14 gestures that can be performed easily and naturally. Finally, we conducted a remote Wizard-of-Oz usability study (N=11) mapping gestures to various daily smartphone operations under a sitting and walking context. From these studies, we develop a taxonomy for mouth gestures, finalize a practical gesture set for common applications, and provide design guidelines for future mouth-based gesture interactions.},
 address = {Virtual Event USA},
 author = {Chen, Victor and Xu, Xuhai and Li, Richard and Shi, Yuanchun and Patel, Shwetak and Wang, Yuntao},
 booktitle = {Proceedings of the {Designing} {Interactive} {Systems} {Conference}},
 doi = {10.1145/3461778.3462004},
 file = {Chen et al. - 2021 - Understanding the Design Space of Mouth Microgestu.pdf:/Users/orsonxu/Zotero/storage/3E7HILPB/Chen et al. - 2021 - Understanding the Design Space of Mouth Microgestu.pdf:application/pdf},
 isbn = {978-1-4503-8476-6},
 language = {en},
 month = {June},
 pages = {1068--1081},
 publisher = {ACM},
 title = {Understanding the {Design} {Space} of {Mouth} {Microgestures}},
 url = {https://dl.acm.org/doi/10.1145/3461778.3462004},
 urldate = {2021-10-07},
 year = {2021}
}

@article{creswell_nightly_2023,
 abstract = {Academic achievement in the first year of college is critical for setting students on a pathway toward long-term academic and life success, yet little is known about the factors that shape early college academic achievement. Given the important role sleep plays in learning and memory, here we extend this work to evaluate whether nightly sleep duration predicts change in end-of-semester grade point average (GPA). First-year college students from three independent universities provided sleep actigraphy for a month early in their winter/spring academic term across five studies. Findings showed that greater early-term total nightly sleep duration predicted higher end-of-term GPA, an effect that persisted even after controlling for previous-term GPA and daytime sleep. Specifically, every additional hour of average nightly sleep duration early in the semester was associated with an 0.07 increase in end-of-term GPA. Sensitivity analyses using sleep thresholds also indicated that sleeping less than 6 h each night was a period where sleep shifted from helpful to harmful for end-of-term GPA, relative to previous-term GPA. Notably, predictive relationships with GPA were specific to total nightly sleep duration, and not other markers of sleep, such as the midpoint of a student’s nightly sleep window or bedtime timing variability. These findings across five studies establish nightly sleep duration as an important factor in academic success and highlight the potential value of testing early academic term total sleep time interventions during the formative first year of college.},
 author = {Creswell, J David and Tumminia, Michael J and Price, Stephen and Sefidgar, Yasaman and Cohen, Sheldon and Ren, Yiyi and Brown, Jennifer and Dey, Anind K and Dutcher, Janine M and Villalba, Daniella K and Mankoff, Jennifer and Xu, Xuhai and Creswell, Kasey and Doryab, Afsaneh and Mattingly, Stephen and Striegel, Aaron and Hachen, David and Martinez, Gonzalo and Lovett, Marsha C},
 doi = {10.1073/pnas.2209123120},
 file = {Creswell et al. - 2023 - Nightly sleep duration predicts grade point averag.pdf:/Users/orsonxu/Zotero/storage/9J4B8U4X/Creswell et al. - 2023 - Nightly sleep duration predicts grade point averag.pdf:application/pdf},
 issn = {0027-8424, 1091-6490},
 journal = {Proceedings of the National Academy of Sciences},
 language = {en},
 month = {February},
 number = {8},
 pages = {e2209123120},
 title = {Nightly sleep duration predicts grade point average in the first year of college},
 url = {https://pnas.org/doi/10.1073/pnas.2209123120},
 urldate = {2023-07-28},
 volume = {120},
 year = {2023}
}

@article{englhardt_classification_2024,
 abstract = {Passively collected behavioral health data from ubiquitous sensors could provide mental health professionals valuable insights into patient's daily lives, but such efforts are impeded by disparate metrics, lack of interoperability, and unclear correlations between the measured signals and an individual's mental health. To address these challenges, we pioneer the exploration of large language models (LLMs) to synthesize clinically relevant insights from multi-sensor data. We develop chain-of-thought prompting methods to generate LLM reasoning on how data pertaining to activity, sleep and social interaction relate to conditions such as depression and anxiety. We then prompt the LLM to perform binary classification, achieving accuracies of 61.1\%, exceeding the state of the art. We find models like GPT-4 correctly reference numerical data 75\% of the time.
While we began our investigation by developing methods to use LLMs to output binary classifications for conditions like depression, we find instead that their greatest potential value to clinicians lies not in diagnostic classification, but rather in rigorous analysis of diverse self-tracking data to generate natural language summaries that synthesize multiple data streams and identify potential concerns. Clinicians envisioned using these insights in a variety of ways, principally for fostering collaborative investigation with patients to strengthen the therapeutic alliance and guide treatment. We describe this collaborative engagement, additional envisioned uses, and associated concerns that must be addressed before adoption in real-world contexts.},
 author = {Englhardt, Zachary and Ma, Chengqian and Morris, Margaret E and Chang, Chun-Cheng and Xu, Xuhai and Qin, Lianhui and McDuff, Daniel and Liu, Xin and Patel, Shwetak and Iyer, Vikram},
 doi = {10.1145/3659604},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/AQNZQBX7/Englhardt et al. - 2024 - From Classification to Clinical Insights Towards Analyzing and Reasoning About Mobile and Behaviora.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {May},
 number = {2},
 pages = {1--25},
 shorttitle = {From {Classification} to {Clinical} {Insights}},
 title = {From {Classification} to {Clinical} {Insights}: {Towards} {Analyzing} and {Reasoning} {About} {Mobile} and {Behavioral} {Health} {Data} {With} {Large} {Language} {Models}},
 url = {https://dl.acm.org/doi/10.1145/3659604},
 urldate = {2024-05-23},
 volume = {8},
 year = {2024}
}

@article{fan_evaluating_2024,
 abstract = {Smartphones hold a great variety of personal data during usage, which at the same time poses privacy risks. In this paper, we used the selling price to reflect users' privacy valuation of their personal data on smartphones. In a 7-day auction, they sold their data as commodities and earn money. We first designed a total of 49 commodities with 8 attributes, covering 14 common types of personal data on smartphones. Then, through a large-scale reverse second price auction (N=181), we examined students' valuation of 15 representative commodities. The average bid-price was 62.8 CNY (8.68 USD) and a regression model with 14 independent variables found the most influential factors for bid-price to be privacy risk, ethnic and gender. When validating our results on non-students (N=34), we found that despite they gave significantly higher prices (M=109.8 CNY, 15.17 USD), "privacy risk" was still one of the most influential factors among the 17 independent variables in the regression model. We recommended that stakeholders should provide 8 attributes of data when selling or managing it.},
 author = {Fan, Lihua and Zhang, Shuning and Kong, Yan and Yi, Xin and Wang, Yang and Xu, Xuhai and Yu, Chun and Li, Hewu and Shi, Yuanchun},
 doi = {10.1145/3678509},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {August},
 number = {3},
 pages = {1--33},
 title = {Evaluating the {Privacy} {Valuation} of {Personal} {Data} on {Smartphones}},
 url = {https://dl.acm.org/doi/10.1145/3678509},
 urldate = {2024-09-23},
 volume = {8},
 year = {2024}
}

@inproceedings{fu_text_2024,
 address = {Honolulu HI USA},
 author = {Fu, Yue and Foell, Sami and Xu, Xuhai and Hiniker, Alexis},
 booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3613904.3641955},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/GJ5A8AAA/Fu et al. - 2024 - From Text to Self Users’ Perception of AIMC Tools on Interpersonal Communication and Self.pdf:application/pdf},
 isbn = {9798400703300},
 language = {en},
 month = {May},
 pages = {1--17},
 publisher = {ACM},
 shorttitle = {From {Text} to {Self}},
 title = {From {Text} to {Self}: {Users}’ {Perception} of {AIMC} {Tools} on {Interpersonal} {Communication} and {Self}},
 url = {https://dl.acm.org/doi/10.1145/3613904.3641955},
 urldate = {2024-05-23},
 year = {2024}
}

@misc{gabriel_can_2024,
 abstract = {Disclaimer: This paper does not endorse the use of large language models (LLMs) for psychotherapy. Our goal is to benchmark equity and quality of care provided by LLMs already deployed in mental health settings.},
 author = {Gabriel, Saadia and Puri, Isha and Xu, Xuhai and Malgaroli, Matteo and Ghassemi, Marzyeh},
 file = {PDF:/Users/orsonxu/Zotero/storage/HRVUKAL6/Gabriel et al. - 2024 - Can AI Relate Testing Large Language Model Response for Mental Health Support.pdf:application/pdf},
 language = {en},
 month = {May},
 note = {arXiv:2405.12021 [cs]},
 publisher = {arXiv},
 shorttitle = {Can {AI} {Relate}},
 title = {Can {AI} {Relate}: {Testing} {Large} {Language} {Model} {Response} for {Mental} {Health} {Support}},
 url = {http://arxiv.org/abs/2405.12021},
 urldate = {2024-09-23},
 year = {2024}
}

@inproceedings{glazko_autoethnographic_2023,
 address = {New York NY USA},
 author = {Glazko, Kate S and Yamagami, Momona and Desai, Aashaka and Mack, Kelly Avery and Potluri, Venkatesh and Xu, Xuhai and Mankoff, Jennifer},
 booktitle = {The 25th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
 doi = {10.1145/3597638.3614548},
 file = {Full Text:/Users/orsonxu/Zotero/storage/XCAXEJN9/Glazko et al. - 2023 - An Autoethnographic Case Study of Generative Artificial Intelligence's Utility for Accessibility.pdf:application/pdf},
 isbn = {9798400702204},
 language = {en},
 month = {October},
 pages = {1--8},
 publisher = {ACM},
 title = {An {Autoethnographic} {Case} {Study} of {Generative} {Artificial} {Intelligence}'s {Utility} for {Accessibility}},
 url = {https://dl.acm.org/doi/10.1145/3597638.3614548},
 urldate = {2024-09-23},
 year = {2023}
}

@article{jin_earcommand_2022,
 abstract = {Intelligent speech interfaces have been developing vastly to support the growing demands for convenient control and interaction with wearable/earable and portable devices. To avoid privacy leakage during speech interactions and strengthen the resistance to ambient noise, silent speech interfaces have been widely explored to enable people's interaction with mobile/wearable devices without audible sounds. However, most existing silent speech solutions require either restricted background illuminations or hand involvement to hold device or perform gestures. In this study, we propose a novel earphone-based, hand-free silent speech interaction approach, named EarCommand. Our technique discovers the relationship between the deformation of the ear canal and the movements of the articulator and takes advantage of this link to recognize different silent speech commands. Our system can achieve a WER (word error rate) of 10.02\% for word-level recognition and 12.33\% for sentence-level recognition, when tested in human subjects with 32 word-level commands and 25 sentence-level commands, which indicates the effectiveness of inferring silent speech commands. Moreover, EarCommand shows high reliability and robustness in a variety of configuration settings and environmental conditions. It is anticipated that EarCommand can serve as an efficient, intelligent speech interface for hand-free operation, which could significantly improve the quality and convenience of interactions.},
 author = {Jin, Yincheng and Gao, Yang and Xu, Xuhai and Choi, Seokmin and Li, Jiyang and Liu, Feng and Li, Zhengxiong and Jin, Zhanpeng},
 doi = {10.1145/3534613},
 file = {Jin et al. - 2022 - EarCommand Hearing Your Silent Speech Commands .pdf:/Users/orsonxu/Zotero/storage/FWSMP8LK/Jin et al. - 2022 - EarCommand Hearing Your Silent Speech Commands .pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {July},
 number = {2},
 pages = {1--28},
 shorttitle = {{EarCommand}},
 title = {{EarCommand}: "{Hearing}" {Your} {Silent} {Speech} {Commands} {In} {Ear}},
 url = {https://dl.acm.org/doi/10.1145/3534613},
 urldate = {2022-11-16},
 volume = {6},
 year = {2022}
}

@article{jin_smartasl_2023,
 abstract = {Sign language builds up an important bridge between the d/Deaf and hard-of-hearing (DHH) and hearing people. Regrettably, most hearing people face challenges in comprehending sign language, necessitating sign language translation. However, state-of-the-art wearable-based techniques mainly concentrate on recognizing manual markers (e.g., hand gestures), while frequently overlooking non-manual markers, such as negative head shaking, question markers, and mouthing. This oversight results in the loss of substantial grammatical and semantic information in sign language. To address this limitation, we introduce SmartASL, a novel proof-of-concept system that can 1) recognize both manual and non-manual markers simultaneously using a combination of earbuds and a wrist-worn IMU, and 2) translate the recognized American Sign Language (ASL) glosses into spoken language. Our experiments demonstrate the SmartASL system's significant potential to accurately recognize the manual and non-manual markers in ASL, effectively bridging the communication gaps between ASL signers and hearing people using commercially available devices.},
 author = {Jin, Yincheng and Zhang, Shibo and Gao, Yang and Xu, Xuhai and Choi, Seokmin and Li, Zhengxiong and Adler, Henry J and Jin, Zhanpeng},
 doi = {10.1145/3596255},
 file = {Jin et al_2023_SmartASL.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Jin et al_2023_SmartASL.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {June},
 number = {2},
 pages = {1--21},
 shorttitle = {{SmartASL}},
 title = {{SmartASL}: "{Point}-of-{Care}" {Comprehensive} {ASL} {Interpreter} {Using} {Wearables}},
 url = {https://dl.acm.org/doi/10.1145/3596255},
 urldate = {2023-07-28},
 volume = {7},
 year = {2023}
}

@misc{kim_adaptive_2024,
 abstract = {Foundation models have become invaluable in advancing the medical field. Despite their promise, the strategic deployment of LLMs for effective utility in complex medical tasks remains an open question. Our novel framework, Medical Decision-making Agents (MDAgents) aims to address this gap by automatically assigning the effective collaboration structure for LLMs. Assigned solo or group collaboration structure is tailored to the complexity of the medical task at hand, emulating real-world medical decision making processes. We evaluate our framework and baseline methods with state-of-the-art LLMs across a suite of challenging medical benchmarks: MedQA, MedMCQA, PubMedQA, DDXPlus, PMC-VQA, Path-VQA, and MedVidQA, achieving the best performance in 5 out of 7 benchmarks that require an understanding of multi-modal medical reasoning. Ablation studies reveal that MDAgents excels in adapting the number of collaborating agents to optimize efficiency and accuracy, showcasing its robustness in diverse scenarios. We also explore the dynamics of group consensus, offering insights into how collaborative agents could behave in complex clinical team dynamics. Our code can be found at https://github.com/mitmedialab/MDAgents.},
 author = {Kim, Yubin and Park, Chanwoo and Jeong, Hyewon and Chan, Yik Siu and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won},
 file = {PDF:/Users/orsonxu/Zotero/storage/H8LKBT8Z/Kim et al. - 2024 - Adaptive Collaboration Strategy for LLMs in Medical Decision Making.pdf:application/pdf},
 language = {en},
 month = {April},
 note = {arXiv:2404.15155 [cs]},
 publisher = {arXiv},
 title = {Adaptive {Collaboration} {Strategy} for {LLMs} in {Medical} {Decision} {Making}},
 url = {http://arxiv.org/abs/2404.15155},
 urldate = {2024-09-23},
 year = {2024}
}

@inproceedings{kim_health-llm_2024,
 abstract = {Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and nonlinguistic data is important. This paper investigates the capacity of LLMs to deliver multimodal health predictions based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of eight state-of-the-art LLMs with diverse prompting and fine-tuning techniques on six public health datasets (PMData, LifeSnaps, GLOBEM, AW\_FB, MITBIH \& MIMIC-III). Our experiments cover thirteen consumer health prediction tasks in mental health, activity, metabolic, sleep, and cardiac assessment. Our fine-tuned model, HealthAlpaca exhibits comparable performance to larger models (GPT-3.5 and GPT-4), achieving the best performance in 5 out of 13 tasks. Ablation studies highlight the effectiveness of context enhancement strategies, and generalization capability of the fine-tuned models across training datasets and the size of training samples. Notably, we observe that our context enhancement can yield up to 23.8\% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance.},
 author = {Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won},
 booktitle = {Proceedings of {Machine} {Learning} {Research}},
 file = {Kim et al. - 2024 - Health-LLM Large Language Models for Health Prediction via Wearable Sensor Data.pdf:/Users/orsonxu/Zotero/storage/XBDN8CGP/Kim et al. - 2024 - Health-LLM Large Language Models for Health Prediction via Wearable Sensor Data.pdf:application/pdf},
 language = {en},
 month = {April},
 note = {arXiv:2401.06866 [cs]},
 publisher = {arXiv},
 shorttitle = {Health-{LLM}},
 title = {Health-{LLM}: {Large} {Language} {Models} for {Health} {Prediction} via {Wearable} {Sensor} {Data}},
 url = {http://arxiv.org/abs/2401.06866},
 urldate = {2024-04-30},
 year = {2024}
}

@article{li_diversense_2022,
 abstract = {The ubiquity of Wi-Fi infrastructure has facilitated the development of a range of Wi-Fi based sensing applications. Wi-Fi sensing relies on weak signal reflections from the human target and thus only supports a limited sensing range, which significantly hinders the real-world deployment of the proposed sensing systems. To extend the sensing range, traditional algorithms focus on suppressing the noise introduced by the imperfect Wi-Fi hardware. This paper picks a different direction and proposes to enhance the quality of the sensing signal by fully exploiting the signal diversity provided by the Wi-Fi hardware. We propose DiverSense, a system that combines sensing signal received from all subcarriers and all antennas in the array, to fully utilize the spatial and frequency diversity. To guarantee the diversity gain after signal combining, we also propose a time-diversity based signal alignment algorithm to align the phase of the multiple received sensing signals. We implement the proposed methods in a respiration monitoring system using commodity Wi-Fi devices and evaluate the performance in diverse environments. Extensive experimental results demonstrate that DiverSense is able to accurately monitor the human respiration even when the sensing signal is under noise floor, and therefore boosts sensing range to 40 meters, which is a 3x improvement over the current state-of-the-art. DiverSense also works robustly under NLoS scenarios, e.g., DiverSense is able to accurately monitor respiration even when the human and the Wi-Fi transceivers are separated by two concrete walls with wooden doors.},
 author = {Li, Yang and Wu, Dan and Zhang, Jie and Xu, Xuhai and Xie, Yaxiong and Gu, Tao and Zhang, Daqing},
 doi = {10.1145/3536393},
 file = {Li et al. - 2022 - DiverSense Maximizing Wi-Fi Sensing Range Leverag.pdf:/Users/orsonxu/Zotero/storage/A8RHQERV/Li et al. - 2022 - DiverSense Maximizing Wi-Fi Sensing Range Leverag.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {July},
 number = {2},
 pages = {1--28},
 shorttitle = {{DiverSense}},
 title = {{DiverSense}: {Maximizing} {Wi}-{Fi} {Sensing} {Range} {Leveraging} {Signal} {Diversity}},
 url = {https://dl.acm.org/doi/10.1145/3536393},
 urldate = {2022-11-16},
 volume = {6},
 year = {2022}
}

@inproceedings{liang_authtrack_2021,
 abstract = {We propose Auth+Track, a novel authentication model that aims to reduce redundant authentication in everyday smartphone usage. By sparse authentication and continuous tracking of the user’s status, Auth+Track eliminates the "gap" authentication between fragmented sessions and enables "Authentication Free when User is Around". To instantiate the Auth+Track model, we present PanoTrack, a prototype that integrates body and near feld hand information for user tracking. We install a fsheye camera on the top of the phone to achieve a panoramic vision that can capture both user’s body and on-screen hands. Based on the captured video stream, we develop an algorithm to extract 1) features for user tracking, including body keypoints and their temporal and spatial association, near feld hand status, and 2) features for user identity assignment. The results of our user studies validate the feasibility of PanoTrack and demonstrate that Auth+Track not only improves the authentication efciency but also enhances user experiences with better usability.},
 address = {Yokohama Japan},
 author = {Liang, Chen and Yu, Chun and Wei, Xiaoying and Xu, Xuhai and Hu, Yongquan and Wang, Yuntao and Shi, Yuanchun},
 booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3411764.3445624},
 file = {Liang et al. - 2021 - Auth+Track Enabling Authentication Free Interacti.pdf:/Users/orsonxu/Zotero/storage/JSZI8L3S/Liang et al. - 2021 - Auth+Track Enabling Authentication Free Interacti.pdf:application/pdf},
 isbn = {978-1-4503-8096-6},
 language = {en},
 month = {May},
 pages = {1--16},
 publisher = {ACM},
 shorttitle = {Auth+{Track}},
 title = {Auth+{Track}: {Enabling} {Authentication} {Free} {Interaction} on {Smartphone} by {Continuous} {User} {Tracking}},
 url = {https://dl.acm.org/doi/10.1145/3411764.3445624},
 urldate = {2021-10-07},
 year = {2021}
}

@misc{liu_harnessing_2024,
 abstract = {Despite the growing interest in leveraging Large Language Models (LLMs) for content analysis, current studies have primarily focused on text-based content. In the present work, we explored the potential of LLMs in assisting video content analysis by conducting a case study that followed a new workflow of LLM-assisted multimodal content analysis. The workflow encompasses codebook design, prompt engineering, LLM processing, and human evaluation. We strategically crafted annotation prompts to get LLM Annotations in structured form and explanation prompts to generate LLM Explanations for a better understanding of LLM reasoning and transparency. To test LLM’s video annotation capabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos about depression. We compared the LLM Annotations with those of two human coders and found that LLM has higher accuracy in object and activity Annotations than emotion and genre Annotations. Moreover, we identified the potential and limitations of LLM’s capabilities in annotating videos. Based on the findings, we explore opportunities and challenges for future research and improvements to the workflow. We also discuss ethical concerns surrounding future studies based on LLM-assisted video analysis.},
 author = {Liu, Jiaying Lizzy and Wang, Yunlong and Lyu, Yao and Su, Yiheng and Niu, Shuo and Xu, Xuhai and Zhang, Yan},
 doi = {10.1145/3678884.3681850},
 file = {PDF:/Users/orsonxu/Zotero/storage/89IV5BB5/Liu et al. - 2024 - Harnessing LLMs for Automated Video Content Analysis An Exploratory Workflow of Short Videos on Dep.pdf:application/pdf},
 language = {en},
 month = {July},
 note = {arXiv:2406.19528 [cs]},
 shorttitle = {Harnessing {LLMs} for {Automated} {Video} {Content} {Analysis}},
 title = {Harnessing {LLMs} for {Automated} {Video} {Content} {Analysis}: {An} {Exploratory} {Workflow} of {Short} {Videos} on {Depression}},
 url = {http://arxiv.org/abs/2406.19528},
 urldate = {2024-09-28},
 year = {2024}
}

@inproceedings{liu_metaphys_2021,
 abstract = {There are large individual differences in physiological processes, making designing personalized health sensing algorithms challenging. Existing machine learning systems struggle to generalize well to unseen subjects or contexts and can often contain problematic biases. Video-based physiological measurement is no exception. Therefore, learning personalized or customized models from a small number of unlabeled samples is very attractive as it would allow fast calibrations to improve generalization and help correct biases. In this paper, we present a novel meta-learning approach called MetaPhys for personalized video-based cardiac measurement for non-contact pulse and heart rate monitoring. Our method uses only 18-seconds of video for customization and works effectively in both supervised and unsupervised manners. We evaluate our approach on two benchmark datasets and demonstrate superior performance in cross-dataset evaluation with substantial reductions (42\% to 44\%) in errors compared with state-of-the-art approaches. We also find that our method leads to large reductions in bias due to skin type.},
 address = {Virtual Event USA},
 author = {Liu, Xin and Jiang, Ziheng and Fromm, Josh and Xu, Xuhai and Patel, Shwetak and McDuff, Daniel},
 booktitle = {Proceedings of the {Conference} on {Health}, {Inference}, and {Learning}},
 doi = {10.1145/3450439.3451870},
 file = {Liu et al. - 2021 - MetaPhys few-shot adaptation for non-contact phys.pdf:/Users/orsonxu/Zotero/storage/6P28KGE9/Liu et al. - 2021 - MetaPhys few-shot adaptation for non-contact phys.pdf:application/pdf},
 isbn = {978-1-4503-8359-2},
 language = {en},
 month = {April},
 pages = {154--163},
 publisher = {ACM},
 shorttitle = {{MetaPhys}},
 title = {{MetaPhys}: few-shot adaptation for non-contact physiological measurement},
 url = {https://dl.acm.org/doi/10.1145/3450439.3451870},
 urldate = {2021-10-07},
 year = {2021}
}

@inproceedings{lu_interactout_2024,
 address = {Honolulu HI USA},
 author = {Lu, Tao and Zheng, Hongxiao and Zhang, Tianying and Xu, Xuhai and Guo, Anhong},
 booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3613904.3642317},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/R56NG6LE/Lu et al. - 2024 - InteractOut Leveraging Interaction Proxies as Input Manipulation Strategies for Reducing Smartphone.pdf:application/pdf},
 isbn = {9798400703300},
 language = {en},
 month = {May},
 pages = {1--19},
 publisher = {ACM},
 shorttitle = {{InteractOut}},
 title = {{InteractOut}: {Leveraging} {Interaction} {Proxies} as {Input} {Manipulation} {Strategies} for {Reducing} {Smartphone} {Overuse}},
 url = {https://dl.acm.org/doi/10.1145/3613904.3642317},
 urldate = {2024-05-23},
 year = {2024}
}

@article{morris_college_2021,
 abstract = {This mixed-method study examined the experiences of college students during the COVID19 pandemic through surveys, experience sampling data collected over two academic quarters (Spring 2019 n1 = 253; Spring 2020 n2 = 147), and semi-structured interviews with 27 undergraduate students. There were no marked changes in mean levels of depressive symptoms, anxiety, stress, or loneliness between 2019 and 2020, or over the course of the Spring 2020 term. Students in both the 2019 and 2020 cohort who indicated psychosocial vulnerability at the initial assessment showed worse psychosocial functioning throughout the entire Spring term relative to other students. However, rates of distress increased faster in 2020 than in 2019 for these individuals. Across individuals, homogeneity of variance tests and multi-level models revealed significant heterogeneity, suggesting the need to examine not just means but the variations in individuals’ experiences. Thematic analysis of interviews characterizes these varied experiences, describing the contexts for students’ challenges and strategies. This analysis highlights the interweaving of psychosocial and academic distress: Challenges such as isolation from peers, lack of interactivity with instructors, and difficulty adjusting to family needs had both an emotional and academic toll. Strategies for adjusting to this new context included initiating remote study and hangout sessions with peers, as well as self-learning. In these and other strategies, students used technologies in different ways and for different purposes than they had previously. Supporting qualitative insight about adaptive responses were quantitative findings that students who used more problem-focused forms of coping reported fewer mental health symptoms over the course of the pandemic, even though they perceived their stress as more severe. These findings underline the need for interventions oriented towards problem-focused coping and suggest opportunities for peer role modeling.},
 author = {Morris, Margaret E and Kuehn, Kevin S and Brown, Jennifer and Nurius, Paula S and Zhang, Han and Sefidgar, Yasaman and Xu, Xuhai and Riskin, Eve A and Dey, Anind K and Consolvo, Sunny and Mankoff, Jennifer},
 doi = {10.1371/journal.pone.0251580},
 editor = {Webster, Amanda A.},
 file = {Morris et al. - 2021 - College from home during COVID-19 A mixed-methods.pdf:/Users/orsonxu/Zotero/storage/WZWX4W9J/Morris et al. - 2021 - College from home during COVID-19 A mixed-methods.pdf:application/pdf},
 issn = {1932-6203},
 journal = {PLOS ONE},
 language = {en},
 month = {June},
 number = {6},
 pages = {e0251580},
 shorttitle = {College from home during {COVID}-19},
 title = {College from home during {COVID}-19: {A} mixed-methods study of heterogeneous experiences},
 url = {https://dx.plos.org/10.1371/journal.pone.0251580},
 urldate = {2021-10-07},
 volume = {16},
 year = {2021}
}

@inproceedings{nemati_ubilung_2022,
 abstract = {Lung health assessment is traditionally done mainly through X-ray images and spirometry tests which are time-consuming, cumbersome, and costly. In this paper, we investigate the potential of passively recordable contents such as speech, cough and heart signal for such an assessment. Our regression model is the first in the literature to achieve mean absolute error (MAE) of 7.47\% for estimation of forced expiratory volume in 1 sec. (FEV1) over forced vital capacity (FVC) ratio using these contents. This is comparable to the state of the art active phone-based spirometry methods. Additionally our classification models achieve a F1-score of 0.982 for healthy v.s. diseased, 0.881 for obstructive v.s. non-obstructive, 0.854 for chronic obstructive pulmonary disease (COPD) v.s. asthma, and 0.892 for severe v.s. non-severe obstruction classification.},
 author = {Nemati, Ebrahim and Xu, Xuhai and Nathan, Viswam and Vatanparvar, Korosh and Ahmed, Tousif and Rahman, Md Mahbubur and McCaffrey, Dan and Kuang, Jilong and Gao, Alex},
 booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
 doi = {10.1109/ICASSP43922.2022.9746614},
 file = {Nemati et al_2022_Ubilung.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Nemati et al_2022_Ubilung.pdf:application/pdf},
 month = {May},
 note = {ISSN: 2379-190X},
 pages = {551--555},
 shorttitle = {Ubilung},
 title = {Ubilung: {Multi}-{Modal} {Passive}-{Based} {Lung} {Health} {Assessment}},
 year = {2022}
}

@inproceedings{nepal_contextual_2024,
 address = {Honolulu HI USA},
 author = {Nepal, Subigya and Pillai, Arvind and Campbell, William and Massachi, Talie and Choi, Eunsol Soul and Xu, Xuhai and Kuc, Joanna and Huckins, Jeremy F and Holden, Jason and Depp, Colin and Jacobson, Nicholas and Czerwinski, Mary P and Granholm, Eric and Campbell, Andrew},
 booktitle = {Extended {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3613905.3650767},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/GLCJ86Z7/Nepal et al. - 2024 - Contextual AI Journaling Integrating LLM and Time Series Behavioral Sensing Technology to Promote S.pdf:application/pdf},
 isbn = {9798400703317},
 language = {en},
 month = {May},
 pages = {1--8},
 publisher = {ACM},
 shorttitle = {Contextual {AI} {Journaling}},
 title = {Contextual {AI} {Journaling}: {Integrating} {LLM} and {Time} {Series} {Behavioral} {Sensing} {Technology} to {Promote} {Self}-{Reflection} and {Well}-being using the {MindScape} {App}},
 url = {https://dl.acm.org/doi/10.1145/3613905.3650767},
 urldate = {2024-05-23},
 year = {2024}
}

@inproceedings{orzikulova_time2stop_2024,
 address = {Honolulu HI USA},
 author = {Orzikulova, Adiba and Xiao, Han and Li, Zhipeng and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Ghassemi, Marzyeh and Lee, Sung-Ju and Dey, Anind K and Xu, Xuhai},
 booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3613904.3642747},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/XKU9J4TD/Orzikulova et al. - 2024 - Time2Stop Adaptive and Explainable Human-AI Loop for Smartphone Overuse Intervention.pdf:application/pdf},
 isbn = {9798400703300},
 language = {en},
 month = {May},
 pages = {1--20},
 publisher = {ACM},
 shorttitle = {{Time2Stop}},
 title = {{Time2Stop}: {Adaptive} and {Explainable} {Human}-{AI} {Loop} for {Smartphone} {Overuse} {Intervention}},
 url = {https://dl.acm.org/doi/10.1145/3613904.3642747},
 urldate = {2024-05-23},
 year = {2024}
}

@inproceedings{qian_fast-forward_2024,
 address = {Honolulu HI USA},
 author = {Qian, Xun and Wang, Tianyi and Xu, Xuhai and Jonker, Tanya R and Todi, Kashyap},
 booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3613904.3642158},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/HB3JL9DV/Qian et al. - 2024 - Fast-Forward Reality Authoring Error-Free Context-Aware Policies with Real-Time Unit Tests in Exten.pdf:application/pdf},
 isbn = {9798400703300},
 language = {en},
 month = {May},
 pages = {1--17},
 publisher = {ACM},
 shorttitle = {Fast-{Forward} {Reality}},
 title = {Fast-{Forward} {Reality}: {Authoring} {Error}-{Free} {Context}-{Aware} {Policies} with {Real}-{Time} {Unit} {Tests} in {Extended} {Reality}},
 url = {https://dl.acm.org/doi/10.1145/3613904.3642158},
 urldate = {2024-05-24},
 year = {2024}
}

@inproceedings{rahman_detecting_2022,
 abstract = {Continuous stress exposure negatively impacts mental and physical well-being. Physiological arousal due to stress affects heartbeat frequency, changes breathing pattern and peripheral temperature, among several other bodily responses. Traditionally stress detection is performed by collecting signals such as electrocardiogram (ECG), respiration, and skin conductance response using uncomfortable sensors such as a chestband. In this study, we use earbuds that passively measure photoplethysmography (PPG), core body temperature, and inertial measurements. We have conducted a lab study exposing 18 participants to an evaluated speech task and additional tasks aimed at increasing stress or promoting relaxation. We simultaneously collected PPG, ECG, impedance cardiography (ICG), and blood pressure using laboratory grade equipment as reference measurements. We show that the earbud PPG sensor can reliably capture heart rate and heart rate variability. We further show that earbud signals can be used to classify the physiological responses associated with stress with 91.30\% recall, 80.52\% precision, and 85.12\% F1-score using a random forest classifier with leave-one-subject-out cross-validation. The accuracy can further be improved through multi-modal sensing. These findings demonstrate the feasibility of using earbuds for passively monitoring users' physiological responses.},
 author = {Rahman, Md Mahbubur and Xu, Xuhai and Nathan, Viswam and Ahmed, Tousif and Ahmed, Mohsin Yusuf and McCaffrey, Dan and Kuang, Jilong and Cowell, Trevor and Moore, Julia and Mendes, Wendy Berry and Gao, Jun Alex},
 booktitle = {2022 44th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} \& {Biology} {Society} ({EMBC})},
 doi = {10.1109/EMBC48229.2022.9871569},
 file = {Rahman et al_2022_Detecting Physiological Responses Using Multimodal Earbud Sensors.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Rahman et al_2022_Detecting Physiological Responses Using Multimodal Earbud Sensors.pdf:application/pdf},
 month = {July},
 pages = {01--05},
 title = {Detecting {Physiological} {Responses} {Using} {Multimodal} {Earbud} {Sensors}},
 year = {2022}
}

@inproceedings{sharif_unlockedmaps_2022,
 abstract = {Current web-based maps do not provide visibility into real-time elevator outages at urban rail transit stations, disenfranchising commuters (e.g., wheelchair users) who rely on functioning elevators at transit stations. In this paper, we demonstrate UnlockedMaps, an open-source and open-data web-based map that visualizes the real-time accessibility of urban rail transit stations in six North American cities, assisting users in making informed decisions regarding their commute. Specifcally, UnlockedMaps uses a map to display transit stations, prominently highlighting their real-time accessibility status (accessible with functioning elevators, accessible but experiencing at least one elevator outage, or not-accessible) and surrounding accessible restaurants and restrooms. UnlockedMaps is the frst system to collect elevator outage data from 2,336 transit stations over 23 months and make it publicly available via an API. We report on results from our pilot user studies with fve stakeholder groups: (1) people with mobility disabilities; (2) pregnant people; (3) cyclists/stroller users/commuters with heavy equipment; (4) members of disability advocacy groups; and (5) civic hackers.},
 address = {Athens Greece},
 author = {Sharif, Ather and Ramesh, Aneesha and Nguyen, Trung-Anh H and Chen, Luna and Zeng, Kent Richard and Hou, Lanqing and Xu, Xuhai},
 booktitle = {The 24th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
 doi = {10.1145/3517428.3550397},
 file = {Sharif et al. - 2022 - UnlockedMaps Visualizing Real-Time Accessibility .pdf:/Users/orsonxu/Zotero/storage/8X7C4P2S/Sharif et al. - 2022 - UnlockedMaps Visualizing Real-Time Accessibility .pdf:application/pdf},
 isbn = {978-1-4503-9258-7},
 language = {en},
 month = {October},
 pages = {1--7},
 publisher = {ACM},
 shorttitle = {{UnlockedMaps}},
 title = {{UnlockedMaps}: {Visualizing} {Real}-{Time} {Accessibility} of {Urban} {Rail} {Transit} {Using} a {Web}-{Based} {Map}},
 url = {https://dl.acm.org/doi/10.1145/3517428.3550397},
 urldate = {2022-11-16},
 year = {2022}
}

@inproceedings{sharif_unlockedmaps_2023,
 address = {Austin TX USA},
 author = {Sharif, Ather and Ramesh, Aneesha and Yu, Qianqian and Nguyen, Trung-Anh H and Xu, Xuhai},
 booktitle = {20th {International} {Web} for {All} {Conference}},
 doi = {10.1145/3587281.3587283},
 file = {Sharif et al_2023_UnlockedMaps.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Sharif et al_2023_UnlockedMaps.pdf:application/pdf},
 isbn = {9798400707483},
 language = {en},
 month = {April},
 pages = {5--17},
 publisher = {ACM},
 shorttitle = {{UnlockedMaps}},
 title = {{UnlockedMaps}: {A} {Web}-{Based} {Map} for {Visualizing} the {Real}-{Time} {Accessibility} of {Urban} {Rail} {Transit} {Stations}},
 url = {https://dl.acm.org/doi/10.1145/3587281.3587283},
 urldate = {2023-07-28},
 year = {2023}
}

@article{wang_g-voila_2024,
 abstract = {Modern information querying systems are progressively incorporating multimodal inputs like vision and audio. However, the integration of gaze --- a modality deeply linked to user intent and increasingly accessible via gaze-tracking wearables --- remains underexplored. This paper introduces a novel gaze-facilitated information querying paradigm, named G-VOILA, which synergizes users' gaze, visual field, and voice-based natural language queries to facilitate a more intuitive querying process. In a user-enactment study involving 21 participants in 3 daily scenarios (p = 21, scene = 3), we revealed the ambiguity in users' query language and a gaze-voice coordination pattern in users' natural query behaviors with G-VOILA. Based on the quantitative and qualitative findings, we developed a design framework for the G-VOILA paradigm, which effectively integrates the gaze data with the in-situ querying context. Then we implemented a G-VOILA proof-of-concept using cutting-edge deep learning techniques. A follow-up user study (p = 16, scene = 2) demonstrates its effectiveness by achieving both higher objective score and subjective score, compared to a baseline without gaze data. We further conducted interviews and provided insights for future gaze-facilitated information querying systems.},
 author = {Wang, Zeyu and Shi, Yuanchun and Wang, Yuntao and Yao, Yuchen and Yan, Kun and Wang, Yuhan and Ji, Lei and Xu, Xuhai and Yu, Chun},
 doi = {10.1145/3659623},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/53ADYFKA/Wang et al. - 2024 - G-VOILA Gaze-Facilitated Information Querying in Daily Scenarios.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {May},
 number = {2},
 pages = {1--33},
 shorttitle = {G-{VOILA}},
 title = {G-{VOILA}: {Gaze}-{Facilitated} {Information} {Querying} in {Daily} {Scenarios}},
 url = {https://dl.acm.org/doi/10.1145/3659623},
 urldate = {2024-05-23},
 volume = {8},
 year = {2024}
}

@article{wang_hearcough_2022,
 abstract = {Cough event detection is the foundation of any measurement associated with cough, one of the primary symptoms of pulmonary illnesses. This paper proposes HearCough, which enables continuous cough event detection on edge computing hearables, by leveraging always-on active noise cancellation (ANC) microphones in commodity hearables. Specifically, we proposed a lightweight end-to-end neural network model — TinyCOUNET and its transfer learning based traning method. When evaluated on our acted cough event dataset, Tiny-COUNET achieved equivalent detection performance but required significantly less computational re­ sources and storage space than cutting-edge cough event detection methods. Then we implemented HearCough by quantifying and deploying the pre-trained Tiny-COUNET to a popular micro-controller in consumer hearables. Lastly, we evaluated that HearCough is effective and reliable for continuous cough event detection through a field study with 8 patients. HearCough achieved 2 Hz cough event detection with an accuracy of 90.0\% and an F1-score of 89.5\% by consuming an additional 5.2 mW power. We envision HearCough as a low-cost add-on for future hearables to enable continuous cough detection and pulmonary health monitoring.},
 author = {Wang, Yuntao and Zhang, Xiyuxing and Chakalasiya, Jay M and Xu, Xuhai and Jiang, Yu and Li, Yuang and Patel, Shwetak and Shi, Yuanchun},
 doi = {10.1016/j.ymeth.2022.05.002},
 file = {Wang et al. - 2022 - HearCough Enabling continuous cough event detecti.pdf:/Users/orsonxu/Zotero/storage/IL9GGM9Z/Wang et al. - 2022 - HearCough Enabling continuous cough event detecti.pdf:application/pdf},
 issn = {10462023},
 journal = {Methods},
 language = {en},
 month = {September},
 pages = {53--62},
 shorttitle = {{HearCough}},
 title = {{HearCough}: {Enabling} continuous cough event detection on edge computing hearables},
 url = {https://linkinghub.elsevier.com/retrieve/pii/S1046202322001165},
 urldate = {2023-07-28},
 volume = {205},
 year = {2022}
}

@inproceedings{wang_modeling_2023,
 address = {Hamburg Germany},
 author = {Wang, Yuntao and Cheng, Zirui and Yi, Xin and Kong, Yan and Wang, Xueyang and Xu, Xuhai and Yan, Yukang and Yu, Chun and Patel, Shwetak and Shi, Yuanchun},
 booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3544548.3581425},
 file = {Wang et al_2023_Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Wang et al_2023_Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images.pdf:application/pdf},
 isbn = {978-1-4503-9421-5},
 language = {en},
 month = {April},
 pages = {1--15},
 publisher = {ACM},
 title = {Modeling the {Trade}-off of {Privacy} {Preservation} and {Activity} {Recognition} on {Low}-{Resolution} {Images}},
 url = {https://dl.acm.org/doi/10.1145/3544548.3581425},
 urldate = {2023-07-28},
 year = {2023}
}

@inproceedings{wen_adaptivevoice_2024,
 address = {Honolulu HI USA},
 author = {Wen, Shaoyue and Ping, Songming and Wang, Jialin and Liang, Hai-Ning and Xu, Xuhai and Yan, Yukang},
 booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3613904.3642876},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/GL5TFNXK/Wen et al. - 2024 - AdaptiveVoice Cognitively Adaptive Voice Interface for Driving Assistance.pdf:application/pdf},
 isbn = {9798400703300},
 language = {en},
 month = {May},
 pages = {1--18},
 publisher = {ACM},
 shorttitle = {{AdaptiveVoice}},
 title = {{AdaptiveVoice}: {Cognitively} {Adaptive} {Voice} {Interface} for {Driving} {Assistance}},
 url = {https://dl.acm.org/doi/10.1145/3613904.3642876},
 urldate = {2024-05-23},
 year = {2024}
}

@inproceedings{wu_lightwrite_2021,
 abstract = {Learning to write is challenging for blind and low vision (BLV) people because of the lack of visual feedback. Regardless of the drastic advancement of digital technology, handwriting is still an essential part of daily life. Although tools designed for teaching BLV to write exist, many are expensive and require the help of sighted teachers. We propose LightWrite, a low-cost, easy-to-access smartphone application that uses voice-based descriptive instruction and feedback to teach BLV users to write English lowercase letters and Arabian digits in a specifically designed font. A two-stage study with 15 BLV users with little prior writing knowledge shows that LightWrite can successfully teach users to learn handwriting characters in an average of 1.09 minutes for each letter. After initial training and 20-minute daily practice for 5 days, participants were able to write an average of 19.9 out of 26 letters that are recognizable by sighted raters.},
 address = {Yokohama Japan},
 author = {Wu, Zihan and Yu, Chun and Xu, Xuhai and Wei, Tong and Zou, Tianyuan and Wang, Ruolin and Shi, Yuanchun},
 booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3411764.3445322},
 file = {Wu et al. - 2021 - LightWrite Teach Handwriting to The Visually Impa.pdf:/Users/orsonxu/Zotero/storage/7I53F874/Wu et al. - 2021 - LightWrite Teach Handwriting to The Visually Impa.pdf:application/pdf},
 isbn = {978-1-4503-8096-6},
 language = {en},
 month = {May},
 pages = {1--15},
 publisher = {ACM},
 shorttitle = {{LightWrite}},
 title = {{LightWrite}: {Teach} {Handwriting} to {The} {Visually} {Impaired} with {A} {Smartphone}},
 url = {https://dl.acm.org/doi/10.1145/3411764.3445322},
 urldate = {2021-10-07},
 year = {2021}
}

@inproceedings{wu_mindshift_2024,
 address = {Honolulu HI USA},
 author = {Wu, Ruolan and Yu, Chun and Pan, Xiaole and Liu, Yujia and Zhang, Ningning and Fu, Yue and Wang, Yuhan and Zheng, Zhi and Chen, Li and Jiang, Qiaolei and Xu, Xuhai and Shi, Yuanchun},
 booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3613904.3642790},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/AFSILKDN/Wu et al. - 2024 - MindShift Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Inter.pdf:application/pdf},
 isbn = {9798400703300},
 language = {en},
 month = {May},
 pages = {1--24},
 publisher = {ACM},
 shorttitle = {{MindShift}},
 title = {{MindShift}: {Leveraging} {Large} {Language} {Models} for {Mental}-{States}-{Based} {Problematic} {Smartphone} {Use} {Intervention}},
 url = {https://dl.acm.org/doi/10.1145/3613904.3642790},
 urldate = {2024-05-23},
 year = {2024}
}

@inproceedings{xu_clench_2019,
 abstract = {People eat every day and biting is one of the most fundamental and natural actions that they perform on a daily basis. Existing work has explored tooth click location and jaw movement as input techniques, however clenching has the potential to add control to this input channel. We propose clench interaction that leverages clenching as an actively controlled physiological signal that can facilitate interactions. We conducted a user study to investigate users’ ability to control their clench force. We found that users can easily discriminate three force levels, and that they can quickly confirm actions by unclenching (quick release). We developed a design space for clench interaction based on the results and investigated the usability of the clench interface. Participants preferred the clench over baselines and indicated a willingness to use clench-based interactions. This novel technique can provide an additional input method in cases where users’ eyes or hands are busy, augment immersive experiences such as virtual/augmented reality, and assist individuals with disabilities.},
 address = {Glasgow Scotland Uk},
 author = {Xu, Xuhai and Yu, Chun and Dey, Anind K and Mankoff, Jennifer},
 booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3290605.3300505},
 file = {Xu et al. - 2019 - Clench Interface Novel Biting Input Techniques.pdf:/Users/orsonxu/Zotero/storage/E3NCFEGR/Xu et al. - 2019 - Clench Interface Novel Biting Input Techniques.pdf:application/pdf},
 isbn = {978-1-4503-5970-2},
 language = {en},
 month = {May},
 pages = {1--12},
 publisher = {ACM},
 shorttitle = {Clench {Interface}},
 title = {Clench {Interface}: {Novel} {Biting} {Input} {Techniques}},
 url = {https://dl.acm.org/doi/10.1145/3290605.3300505},
 urldate = {2021-10-07},
 year = {2019}
}

@inproceedings{xu_earbuddy_2020,
 abstract = {Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classiﬁcation. Our optimized classiﬁer achieved an accuracy of 95.3\%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability.},
 address = {Honolulu HI USA},
 author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, Wenjia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K},
 booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 file = {Xu et al. - EarBuddy Enabling On-Face Interaction via Wireles.pdf:/Users/orsonxu/Zotero/storage/6BXBWNYE/Xu et al. - EarBuddy Enabling On-Face Interaction via Wireles.pdf:application/pdf},
 language = {en},
 pages = {14},
 publisher = {ACM},
 title = {{EarBuddy}: {Enabling} {On}-{Face} {Interaction} via {Wireless} {Earbuds}},
 year = {2020}
}

@inproceedings{xu_enabling_2022,
 abstract = {We present a framework for gesture customization requiring minimal examples from users, all without degrading the performance of existing gesture sets. To achieve this, we frst deployed a large-scale study (N=500+) to collect data and train an accelerometer-gyroscope recognition model with a cross-user accuracy of 95.7\% and a falsepositive rate of 0.6 per hour when tested on everyday non-gesture data. Next, we design a few-shot learning framework which derives a lightweight model from our pre-trained model, enabling knowledge transfer without performance degradation. We validate our approach through a user study (N=20) examining on-device customization from 12 new gestures, resulting in an average accuracy of 55.3\%, 83.1\%, and 87.2\% on using one, three, or fve shots when adding a new gesture, while maintaining the same recognition accuracy and false-positive rate from the pre-existing gesture set. We further evaluate the usability of our real-time implementation with a user experience study (N=20). Our results highlight the efectiveness, learnability, and usability of our customization framework.},
 address = {New Orleans LA USA},
 author = {Xu, Xuhai and Gong, Jun and Brum, Carolina and Liang, Lilian and Suh, Bongsoo and Gupta, Shivam Kumar and Agarwal, Yash and Lindsey, Laurence and Kang, Runchang and Shahsavari, Behrooz and Nguyen, Tu and Nieto, Heriberto and Hudson, Scott E and Maalouf, Charlie and Mousavi, Jax Seyed and Laput, Gierad},
 booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3491102.3501904},
 file = {Xu et al. - 2022 - Enabling Hand Gesture Customization on Wrist-Worn .pdf:/Users/orsonxu/Zotero/storage/XUC3K2JT/Xu et al. - 2022 - Enabling Hand Gesture Customization on Wrist-Worn .pdf:application/pdf},
 isbn = {978-1-4503-9157-3},
 language = {en},
 month = {April},
 pages = {1--19},
 publisher = {ACM},
 title = {Enabling {Hand} {Gesture} {Customization} on {Wrist}-{Worn} {Devices}},
 url = {https://dl.acm.org/doi/10.1145/3491102.3501904},
 urldate = {2022-07-10},
 year = {2022}
}

@inproceedings{xu_globem_2022,
 abstract = {Recent research has demonstrated the capability of behavior signals captured by smartphones and wearables for longitudinal behavior modeling. However, there is a lack of a comprehensive public dataset that serves as an open testbed for fair comparison among algorithms. Moreover, prior studies mainly evaluate algorithms using data from a single population within a short period, without measuring the cross-dataset generalizability of these algorithms. We present the first multi-year passive sensing datasets, containing over 700 user-years and 497 unique users’ data collected from mobile and wearable sensors, together with a wide range of well-being metrics. Our datasets can support multiple cross-dataset evaluations of behavior modeling algorithms’ generalizability across different users and years. As a starting point, we provide the benchmark results of 18 algorithms on the task of depression detection. Our results indicate that both prior depression detection algorithms and domain generalization techniques show potential but need further research to achieve adequate cross-dataset generalizability. We envision our multi-year datasets can support the ML community in developing generalizable longitudinal behavior modeling algorithms.},
 author = {Xu, Xuhai and Zhang, Han and Sefidgar, Yasaman and Ren, Yiyi and Liu, Xin and Seo, Woosuk and Brown, Jennifer and Kuehn, Kevin S and Merrill, Mike and Nurius, Paula S and Patel, Shwetak and Althoff, Tim and Morris, Margaret E and Riskin, Eve A and Mankoff, Jennifer and Dey, Anind K},
 booktitle = {Thirty-sixth {Conference} on {Neural} {Information} {Processing} {Systems} {Datasets} and {Benchmarks} {Track}},
 file = {Xu et al. - GLOBEM Dataset Multi-Year Datasets for Longitudin.pdf:/Users/orsonxu/Zotero/storage/3K7MCSA6/Xu et al. - GLOBEM Dataset Multi-Year Datasets for Longitudin.pdf:application/pdf},
 language = {en},
 pages = {18},
 title = {{GLOBEM} {Dataset}: {Multi}-{Year} {Datasets} for {Longitudinal} {Human} {Behavior} {Modeling} {Generalization}},
 year = {2022}
}

@article{xu_globem_2023,
 abstract = {There is a growing body of research revealing that longitudinal passive sensing data from smartphones and wearable devices can capture daily behavior signals for human behavior modeling, such as depression detection. Most prior studies build and evaluate machine learning models using data collected from a single population. However, to ensure that a behavior model can work for a larger group of users, its generalizability needs to be verified on multiple datasets from different populations. We present the first work evaluating cross-dataset generalizability of longitudinal behavior models, using depression detection as an application. We collect multiple longitudinal passive mobile sensing datasets with over 500 users from two institutes over a two-year span, leading to four institute-year datasets. Using the datasets, we closely re-implement and evaluated nine prior depression detection algorithms. Our experiment reveals the lack of model generalizability of these methods. We also implement eight recently popular domain generalization algorithms from the machine learning community. Our results indicate that these methods also do not generalize well on our datasets, with barely any advantage over the naive baseline of guessing the majority. We then present two new algorithms with better generalizability. Our new algorithm, Reorder, significantly and consistently outperforms existing methods on most cross-dataset generalization setups. However, the overall advantage is incremental and still has great room for improvement. Our analysis reveals that the individual differences (both within and between populations) may play the most important role in the cross-dataset generalization challenge. Finally, we provide an open-source benchmark platform GLOBEM- short for Generalization of Longitudinal BEhavior Modeling - to consolidate all 19 algorithms. GLOBEM can support researchers in using, developing, and evaluating different longitudinal behavior modeling methods. We call for researchers' attention to model generalizability evaluation for future longitudinal human behavior modeling studies.},
 author = {Xu, Xuhai and Liu, Xin and Zhang, Han and Wang, Weichen and Nepal, Subigya and Sefidgar, Yasaman and Seo, Woosuk and Kuehn, Kevin S and Huckins, Jeremy F and Morris, Margaret E and Nurius, Paula S and Riskin, Eve A and Patel, Shwetak and Althoff, Tim and Campbell, Andrew and Dey, Anind K and Mankoff, Jennifer},
 doi = {10.1145/3569485},
 file = {Xu et al_2022_GLOBEM.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Xu et al_2022_GLOBEM.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 number = {4},
 pages = {1--34},
 shorttitle = {{GLOBEM}},
 title = {{GLOBEM}: {Cross}-{Dataset} {Generalization} of {Longitudinal} {Human} {Behavior} {Modeling}},
 url = {https://dl.acm.org/doi/10.1145/3569485},
 urldate = {2023-08-04},
 volume = {6},
 year = {2023}
}

@inproceedings{xu_hulamove_2021,
 abstract = {We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We frst conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confrm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5\%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove signifcantly reduced interaction time by 41.8\% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.},
 address = {Yokohama Japan},
 author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K},
 booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3411764.3445182},
 file = {Xu et al. - 2021 - HulaMove Using Commodity IMU for Waist Interactio.pdf:/Users/orsonxu/Zotero/storage/J773T8NJ/Xu et al. - 2021 - HulaMove Using Commodity IMU for Waist Interactio.pdf:application/pdf},
 isbn = {978-1-4503-8096-6},
 language = {en},
 month = {May},
 pages = {1--16},
 publisher = {ACM},
 shorttitle = {{HulaMove}},
 title = {{HulaMove}: {Using} {Commodity} {IMU} for {Waist} {Interaction}},
 url = {https://dl.acm.org/doi/10.1145/3411764.3445182},
 urldate = {2021-08-04},
 year = {2021}
}

@article{xu_leveraging_2019,
 abstract = {The rate of depression in college students is rising, which is known to increase suicide risk, lower academic performance and double the likelihood of dropping out of school. Existing work on fnding relationships between passively sensed behavior and depression, as well as detecting depression, mainly derives relevant unimodal features from a single sensor. However, co-occurrence of values in multiple sensors may provide better features, because such features can describe behavior in context. We present a new method to extract contextually fltered features from passively collected, time-series mobile data via association rule mining. After calculating traditional unimodal features from the data, we extract rules that relate unimodal features to each other using association rule mining. We extract rules from each class separately (e.g., depression vs. nondepression). We introduce a new metric to select a subset of rules that distinguish between the two classes. From these rules, which capture the relationship between multiple unimodal features, we automatically extract contextually fltered features. These features are then fed into a traditional machine learning pipeline to detect the class of interest (in our case, depression), defned by whether a student has a high BDI-II score at the end of the semester. The behavior rules generated by our methods are highly interpretable representations of diferences between classes. Our best model uses contextually-fltered features to signifcantly outperform a standard model that uses only unimodal features, by an average of 9.7\% across a variety of metrics. We further verifed the generalizability of our approach on a second dataset, and achieved very similar results. CCS Concepts: • Human-centered computing Ubiquitous and mobile computing; • Applied computing Life and medical sciences.},
 author = {Xu, Xuhai and Chikersal, Prerna and Doryab, Afsaneh and Villalba, Daniella K and Dutcher, Janine M and Tumminia, Michael J and Althoff, Tim and Cohen, Sheldon and Creswell, Kasey and Creswell, J David and Mankoff, Jennifer and Dey, Anind K},
 doi = {10.1145/3351274},
 file = {Xu et al. - 2019 - Leveraging Routine Behavior and Contextually-Filte.pdf:/Users/orsonxu/Zotero/storage/UGVK8ESA/Xu et al. - 2019 - Leveraging Routine Behavior and Contextually-Filte.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {September},
 number = {3},
 pages = {1--33},
 title = {Leveraging {Routine} {Behavior} and {Contextually}-{Filtered} {Features} for {Depression} {Detection} among {College} {Students}},
 url = {https://dl.acm.org/doi/10.1145/3351274},
 urldate = {2021-10-07},
 volume = {3},
 year = {2019}
}

@article{xu_leveraging_2021,
 abstract = {The prevalence of mobile phones and wearable devices enables the passive capturing and modeling of human behavior at an unprecedented resolution and scale. Past research has demonstrated the capability of mobile sensing to model aspects of physical health, mental health, education, and work performance, etc. However, most of the algorithms and models proposed in previous work follow a one-size-fits-all (i.e., population modeling) approach that looks for common behaviors amongst all users, disregarding the fact that individuals can behave very differently, resulting in reduced model performance. Further, black-box models are often used that do not allow for interpretability and human behavior understanding. We present a new method to address the problems of personalized behavior classification and interpretability, and apply it to depression detection among college students. Inspired by the idea of collaborative-filtering, our method is a type of memory-based learning algorithm. It leverages the relevance of mobile-sensed behavior features among individuals to calculate personalized relevance weights, which are used to impute missing data and select features according to a specific modeling goal (e.g., whether the student has depressive symptoms) in different time epochs, i.e., times of the day and days of the week. It then compiles features from epochs using majority voting to obtain the final prediction. We apply our algorithm on a depression detection dataset collected from first-year college students with low data-missing rates and show that our method outperforms the state-of-the-art machine learning model by 5.1\% in accuracy and 5.5\% in F1 score. We further verify the pipeline-level generalizability of our approach by achieving similar results on a second dataset, with an average improvement of 3.4\% across performance metrics. Beyond achieving better classification performance, our novel approach is further able to generate personalized interpretations of the models for each individual. These interpretations are supported by existing depression-related literature and can potentially inspire automated and personalized depression intervention design in the future.},
 author = {Xu, Xuhai and Chikersal, Prerna and Dutcher, Janine M and Sefidgar, Yasaman and Seo, Woosuk and Tumminia, Michael J and Villalba, Daniella K and Cohen, Sheldon and Creswell, Kasey and Creswell, J David and Doryab, Afsaneh and Nurius, Paula S and Riskin, Eve A and Dey, Anind K and Mankoff, Jennifer},
 doi = {10.1145/3448107},
 file = {Xu et al. - 2021 - Leveraging Collaborative-Filtering for Personalize.pdf:/Users/orsonxu/Zotero/storage/AX2ZQTNL/Xu et al. - 2021 - Leveraging Collaborative-Filtering for Personalize.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {March},
 number = {1},
 pages = {1--27},
 shorttitle = {Leveraging {Collaborative}-{Filtering} for {Personalized} {Behavior} {Modeling}},
 title = {Leveraging {Collaborative}-{Filtering} for {Personalized} {Behavior} {Modeling}: {A} {Case} {Study} of {Depression} {Detection} among {College} {Students}},
 url = {https://dl.acm.org/doi/10.1145/3448107},
 urldate = {2021-10-07},
 volume = {5},
 year = {2021}
}

@article{xu_listen2cough_2021,
 abstract = {The prevalence of ubiquitous computing enables new opportunities for lung health monitoring and assessment. In the past few years, there have been extensive studies on cough detection using passively sensed audio signals. However, the generalizability of a cough detection model when applied to external datasets, especially in real-world implementation, is questionable and not explored adequately. Beyond detecting coughs, researchers have looked into how cough sounds can be used in assessing lung health. However, due to the challenges in collecting both cough sounds and lung health condition ground truth, previous studies have been hindered by the limited datasets. In this paper, we propose Listen2Cough to address these gaps. We first build an end-to-end deep learning architecture using public cough sound datasets to detect coughs within raw audio recordings. We employ a pre-trained MobileNet and integrate a number of augmentation techniques to improve the generalizability of our model. Without additional fine-tuning, our model is able to achieve an F1 score of 0.948 when tested against a new clean dataset, and 0.884 on another in-the-wild noisy dataset, leading to an advantage of 5.8\% and 8.4\% on average over the best baseline model, respectively. Then, to mitigate the issue of limited lung health data, we propose to transform the cough detection task to lung health assessment tasks so that the rich cough data can be leveraged. Our hypothesis is that these tasks extract and utilize similar effective representation from cough sounds. We embed the cough detection model into a multi-instance learning framework with the attention mechanism and further tune the model for lung health assessment tasks. Our final model achieves an F1-score of 0.912 on healthy v.s. unhealthy, 0.870 on obstructive v.s. non-obstructive, and 0.813 on COPD v.s. asthma classification, outperforming the baseline by 10.7\%, 6.3\%, and 3.7\%, respectively. Moreover, the weight value in the attention layer can be used to identify important coughs highly correlated with lung health, which can potentially provide interpretability for expert diagnosis in the future. CCS Concepts: • Human-centered computing → Ubiquitous and mobile computing; • Applied computing → Life and medical sciences.},
 author = {Xu, Xuhai and Nemati, Ebrahim and Vatanparvar, Korosh and Nathan, Viswam and Ahmed, Tousif and Rahman, Md Mahbubur and McCaffrey, Daniel and Kuang, Jilong and Gao, Jun Alex},
 doi = {10.1145/3448124},
 file = {Xu et al. - 2021 - Listen2Cough Leveraging End-to-End Deep Learning .pdf:/Users/orsonxu/Zotero/storage/ACPJMCBB/Xu et al. - 2021 - Listen2Cough Leveraging End-to-End Deep Learning .pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {March},
 number = {1},
 pages = {1--22},
 shorttitle = {{Listen2Cough}},
 title = {{Listen2Cough}: {Leveraging} {End}-to-{End} {Deep} {Learning} {Cough} {Detection} {Model} to {Enhance} {Lung} {Health} {Assessment} {Using} {Passively} {Sensed} {Audio}},
 url = {https://dl.acm.org/doi/10.1145/3448124},
 urldate = {2021-10-07},
 volume = {5},
 year = {2021}
}

@article{xu_mental-llm_2024,
 abstract = {Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9\% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8\%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.},
 author = {Xu, Xuhai and Yao, Bingsheng and Dong, Yuanzhe and Gabriel, Saadia and Yu, Hong and Hendler, James and Ghassemi, Marzyeh and Dey, Anind K and Wang, Dakuo},
 doi = {10.1145/3643540},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/CY75CQWP/Xu et al. - 2024 - Mental-LLM Leveraging Large Language Models for Mental Health Prediction via Online Text Data.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {March},
 number = {1},
 pages = {1--32},
 shorttitle = {Mental-{LLM}},
 title = {Mental-{LLM}: {Leveraging} {Large} {Language} {Models} for {Mental} {Health} {Prediction} via {Online} {Text} {Data}},
 url = {https://dl.acm.org/doi/10.1145/3643540},
 urldate = {2024-05-23},
 volume = {8},
 year = {2024}
}

@article{xu_recognizing_2020,
 abstract = {A multi-touch interactive tabletop is designed to embody the benefits of a digital computer within the familiar surface of a physical tabletop. However, the nature of current multi-touch tabletops to detect and react to all forms of touch, including unintentional touches, impedes users from acting naturally on them. In our research, we leverage gaze direction, head orientation and screen contact data to identify and filter out unintentional touches, so that users can take full advantage of the physical properties of an interactive tabletop, e.g., resting hands or leaning on the tabletop during the interaction. To achieve this, we first conducted a user study to identify behavioral pattern differences (gaze, head and touch) between completing usual tasks on digital versus physical tabletops. We then compiled our findings into five types of spatiotemporal features, and train a machine learning model to recognize unintentional touches with an F1 score of 91.3\%, outperforming the state-of-the-art model by 4.3\%. Finally we evaluated our algorithm in a real-time filtering system. A user study shows that our algorithm is stable and the improved tabletop effectively screens out unintentional touches, and provide more relaxing and natural user experience. By linking their gaze and head behavior to their touch behavior, our work sheds light on the possibility of future tabletop technology to improve the understanding of users' input intention.},
 author = {Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun},
 doi = {10.1145/3381011},
 file = {Xu et al. - 2020 - Recognizing Unintentional Touch on Interactive Tab.pdf:/Users/orsonxu/Zotero/storage/SNGMSGNT/Xu et al. - 2020 - Recognizing Unintentional Touch on Interactive Tab.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {March},
 number = {1},
 pages = {1--24},
 title = {Recognizing {Unintentional} {Touch} on {Interactive} {Tabletop}},
 url = {https://dl.acm.org/doi/10.1145/3381011},
 urldate = {2021-10-07},
 volume = {4},
 year = {2020}
}

@inproceedings{xu_towards_2022,
 address = {Bend OR USA},
 author = {Xu, Xuhai},
 booktitle = {Adjunct {Proceedings} of the 35th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
 doi = {10.1145/3526114.3558524},
 file = {Xu_2022_Towards Future Health and Well-being.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Xu_2022_Towards Future Health and Well-being.pdf:application/pdf},
 isbn = {978-1-4503-9321-8},
 language = {en},
 month = {October},
 pages = {1--5},
 publisher = {ACM},
 shorttitle = {Towards {Future} {Health} and {Well}-being},
 title = {Towards {Future} {Health} and {Well}-being: {Bridging} {Behavior} {Modeling} and {Intervention}},
 url = {https://dl.acm.org/doi/10.1145/3526114.3558524},
 urldate = {2023-07-28},
 year = {2022}
}

@inproceedings{xu_typeout_2022,
 abstract = {Smartphone overuse is related to a variety of issues such as lack of sleep and anxiety. We explore the application of Self-Afrmation Theory on smartphone overuse intervention in a just-in-time manner. We present TypeOut, a just-in-time intervention technique that integrates two components: an in-situ typing-based unlock process to improve user engagement, and self-afrmation-based typing content to enhance efectiveness. We hypothesize that the integration of typing and self-afrmation content can better reduce smartphone overuse. We conducted a 10-week within-subject feld experiment (N=54) and compared TypeOut against two baselines: one only showing the self-afrmation content (a common notifcation-based intervention), and one only requiring typing non-semantic content (a state-of-the-art method). TypeOut reduces app usage by over 50\%, and both app opening frequency and usage duration by over 25\%, all signifcantly outperforming baselines. TypeOut can potentially be used in other domains where an intervention may beneft from integrating self-afrmation exercises with an engaging just-in-time mechanism.},
 address = {New Orleans LA USA},
 author = {Xu, Xuhai and Zou, Tianyuan and Xiao, Han and Li, Yanzhang and Wang, Ruolin and Yuan, Tianyi and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K},
 booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3491102.3517476},
 file = {Xu et al. - 2022 - TypeOut Leveraging Just-in-Time Self-Affirmation .pdf:/Users/orsonxu/Zotero/storage/TA6VE4BN/Xu et al. - 2022 - TypeOut Leveraging Just-in-Time Self-Affirmation .pdf:application/pdf},
 isbn = {978-1-4503-9157-3},
 language = {en},
 month = {April},
 pages = {1--17},
 publisher = {ACM},
 shorttitle = {{TypeOut}},
 title = {{TypeOut}: {Leveraging} {Just}-in-{Time} {Self}-{Affirmation} for {Smartphone} {Overuse} {Reduction}},
 url = {https://dl.acm.org/doi/10.1145/3491102.3517476},
 urldate = {2022-07-10},
 year = {2022}
}

@inproceedings{xu_understanding_2020,
 abstract = {Personalized document recommendation systems aim to provide users with a quick shortcut to the documents they may want to access next, usually with an explanation about why the document is recommended. Previous work explored various methods for better recommendations and better explanations in different domains. However, there are few efforts that closely study how users react to the recommended items in a document recommendation scenario. We conducted a large-scale log study of users’ interaction behavior with the explainable recommendation on one of the largest cloud document platforms office.com. Our analysis reveals a number of factors, including display position, file type, authorship, recency of last access, and most importantly, the recommendation explanations, that are associated with whether users will recognize or open the recommended documents. Moreover, we specifically focus on explanations and conduct an online experiment to investigate the influence of different explanations on user behavior. Our analysis indicates that the recommendations help users access their documents significantly faster, but sometimes users miss a recommendation and resort to other more complicated methods to open the documents. Our results suggest opportunities to improve explanations and more generally the design of systems that provide and explain recommendations for documents.},
 address = {Taipei Taiwan},
 author = {Xu, Xuhai and Awadallah, Ahmed Hassan and Dumais, Susan T and Omar, Farheen and Popp, Bogdan and Rounthwaite, Robert and Jahanbakhsh, Farnaz},
 booktitle = {Proceedings of {The} {Web} {Conference}},
 doi = {10.1145/3366423.3380071},
 file = {Xu et al. - 2020 - Understanding User Behavior For Document Recommend.pdf:/Users/orsonxu/Zotero/storage/S6FXIJMC/Xu et al. - 2020 - Understanding User Behavior For Document Recommend.pdf:application/pdf},
 isbn = {978-1-4503-7023-3},
 language = {en},
 month = {April},
 pages = {3012--3018},
 publisher = {ACM},
 title = {Understanding {User} {Behavior} {For} {Document} {Recommendation}},
 url = {https://dl.acm.org/doi/10.1145/3366423.3380071},
 urldate = {2021-10-07},
 year = {2020}
}

@article{xu_understanding_2021,
 abstract = {Passive mobile sensing for the purpose of human state modeling is a fast-growing area. It has been applied to solve a wide range of behavior-related problems, including physical and mental health monitoring, affective computing, activity recognition, routine modeling, etc. However, in spite of the emerging literature that has investigated a wide range of application scenarios, there is little work focusing on the lessons learned by researchers, and on guidance for researchers to this approach. How do researchers conduct these types of research studies? Is there any established common practice when applying mobile sensing across different application areas? What are the pain points and needs that they frequently encounter? Answering these questions is an important step in the maturing of this growing sub-field of ubiquitous computing, and can benefit a wide range of audiences. It can serve to educate researchers who have growing interests in this area but have little to no previous experience. Intermediate researchers may also find the results interesting and helpful for reference to improve their skills. Moreover, it can further shed light on the design guidelines for a future toolkit that could facilitate research processes being used. In this paper, we fill this gap and answer these questions by conducting semi-structured interviews with ten experienced researchers from four countries to understand their practices and pain points when conducting their research. Our results reveal a common pipeline that researchers have adopted, and identify major challenges that do not appear in published work but that researchers often encounter. Based on the results of our interviews, we discuss practical suggestions for novice researchers and high-level design principles for a toolkit that can accelerate passive mobile sensing research.},
 author = {Xu, Xuhai and Mankoff, Jennifer and Dey, Anind K},
 doi = {10.1007/s42486-021-00072-4},
 file = {Xu et al. - 2021 - Understanding practices and needs of researchers i.pdf:/Users/orsonxu/Zotero/storage/BCU75N2D/Xu et al. - 2021 - Understanding practices and needs of researchers i.pdf:application/pdf},
 issn = {2524-521X, 2524-5228},
 journal = {CCF Transactions on Pervasive Computing and Interaction},
 language = {en},
 month = {July},
 title = {Understanding practices and needs of researchers in human state modeling by passive mobile sensing},
 url = {https://link.springer.com/10.1007/s42486-021-00072-4},
 urldate = {2021-09-30},
 year = {2021}
}

@inproceedings{xu_xair_2023,
 address = {Hamburg Germany},
 author = {Xu, Xuhai and Yu, Anna and Jonker, Tanya R and Todi, Kashyap and Lu, Feiyu and Qian, Xun and Belo, Joao Marcelo Evangelista and Wang, Tianyi and Li, Michelle and Mun, Aran and Wu, Te-Yen and Shen, Junxiao and Zhang, Ting and Kokhlikyan, Narine and Wang, Fulton and Sorenson, Paul and Kim, Sophie and Benko, Hrvoje},
 booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3544548.3581500},
 file = {Xu et al_2023_XAIR.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Xu et al_2023_XAIR.pdf:application/pdf},
 isbn = {978-1-4503-9421-5},
 language = {en},
 month = {April},
 pages = {1--30},
 publisher = {ACM},
 shorttitle = {{XAIR}},
 title = {{XAIR}: {A} {Framework} of {Explainable} {AI} in {Augmented} {Reality}},
 url = {https://dl.acm.org/doi/10.1145/3544548.3581500},
 urldate = {2023-07-28},
 year = {2023}
}

@article{yan_conespeech_2023,
 abstract = {Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.},
 author = {Yan, Yukang and Liu, Haohua and Shi, Yingtian and Wang, Jingying and Guo, Ruici and Li, Zisu and Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun},
 doi = {10.1109/TVCG.2023.3247085},
 file = {Yan et al. - 2023 - ConeSpeech Exploring Directional Speech Interacti.pdf:/Users/orsonxu/Zotero/storage/5KW584VF/Yan et al. - 2023 - ConeSpeech Exploring Directional Speech Interacti.pdf:application/pdf},
 issn = {1077-2626, 1941-0506, 2160-9306},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 language = {en},
 month = {May},
 number = {5},
 pages = {2647--2657},
 shorttitle = {{ConeSpeech}},
 title = {{ConeSpeech}: {Exploring} {Directional} {Speech} {Interaction} for {Multi}-{Person} {Remote} {Communication} in {Virtual} {Reality}},
 url = {https://ieeexplore.ieee.org/document/10049667/},
 urldate = {2023-07-28},
 volume = {29},
 year = {2023}
}

@inproceedings{yan_frownonerror_2020,
 abstract = {In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a ﬁrst user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the signiﬁcant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efﬁciency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4\%, recall: 97.6\%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.},
 address = {Honolulu HI USA},
 author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun},
 booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3313831.3376810},
 file = {Yan et al. - 2020 - FrownOnError Interrupting Responses from Smart Sp.pdf:/Users/orsonxu/Zotero/storage/YM9NFMSS/Yan et al. - 2020 - FrownOnError Interrupting Responses from Smart Sp.pdf:application/pdf},
 isbn = {978-1-4503-6708-0},
 language = {en},
 month = {April},
 pages = {1--14},
 publisher = {ACM},
 shorttitle = {{FrownOnError}},
 title = {{FrownOnError}: {Interrupting} {Responses} from {Smart} {Speakers} by {Facial} {Expressions}},
 url = {https://dl.acm.org/doi/10.1145/3313831.3376810},
 urldate = {2021-10-07},
 year = {2020}
}

@article{yang_talk2care_2024,
 abstract = {Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults' conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers' efforts and time. We envision our work as an initial exploration of LLMs' capability in the intersection of healthcare and interpersonal communication.},
 author = {Yang, Ziqi and Xu, Xuhai and Yao, Bingsheng and Rogers, Ethan and Zhang, Shao and Intille, Stephen and Shara, Nawar and Gao, Guodong Gordon and Wang, Dakuo},
 doi = {10.1145/3659625},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/4VM647A7/Yang et al. - 2024 - Talk2Care An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adu.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {May},
 number = {2},
 pages = {1--35},
 shorttitle = {{Talk2Care}},
 title = {{Talk2Care}: {An} {LLM}-based {Voice} {Assistant} for {Communication} between {Healthcare} {Providers} and {Older} {Adults}},
 url = {https://dl.acm.org/doi/10.1145/3659625},
 urldate = {2024-05-23},
 volume = {8},
 year = {2024}
}

@inproceedings{yao_reviewing_2023,
 address = {Hamburg Germany},
 author = {Yao, Yuan and Huang, Li and He, Yi and Ma, Zhijun and Xu, Xuhai and Mi, Haipeng},
 booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3544548.3580842},
 file = {Yao et al_2023_Reviewing and Reflecting on Smart Home Research from the Human-Centered Perspective.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Yao et al_2023_Reviewing and Reflecting on Smart Home Research from the Human-Centered Perspective.pdf:application/pdf},
 isbn = {978-1-4503-9421-5},
 language = {en},
 month = {April},
 pages = {1--21},
 publisher = {ACM},
 title = {Reviewing and {Reflecting} on {Smart} {Home} {Research} from the {Human}-{Centered} {Perspective}},
 url = {https://dl.acm.org/doi/10.1145/3544548.3580842},
 urldate = {2023-07-28},
 year = {2023}
}

@inproceedings{zhang_boldmove_2022,
 abstract = {Recent advances in ultra-low-power ubiquitous touch interfaces make touch inputs possible anytime, anywhere. However, their functions are usually pre-determined, i.e., one button is only associated with one fixed function. BoldMove enables spontaneous and efficient association of touch inputs and IoT device functions with semantic-based function filtering and a wait-confirm sequential selection strategy. In this way, such touch interfaces become ubiquitous IoT device controllers. We proposed the semantic-based IoT function filtering to improve control efficiency, then designed the sequential selection mechanism for interfaces with constrained input and output resources. We implemented BoldMove on a custom-built touch interface with capacitive button inputs and a smartwatch display. We then conducted a user study to determine the design parameters for the sequential selection method. At last, we validated that BoldMove only takes 3.25 seconds to complete a selection task if the target function appears within the Top-3 displayed item. Even if the assumption is relaxed to Top-10, BoldMove is still estimated to be more efficient than the conventional selection method with device-based filtering and menu-navigated selection.},
 address = {New Orleans LA USA},
 author = {Zhang, Tengxiang and Zeng, Xin and Zhang, Yinshuai and Jiang, Xin and Xu, Xuhai and Dey, Anind K and Chen, Yiqiang},
 booktitle = {Extended {Abstracts} {Proceedings} of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3491101.3519805},
 file = {Zhang et al. - 2022 - BoldMove Enabling IoT Device Control on Ubiquitou.pdf:/Users/orsonxu/Zotero/storage/TH7P7844/Zhang et al. - 2022 - BoldMove Enabling IoT Device Control on Ubiquitou.pdf:application/pdf},
 isbn = {978-1-4503-9156-6},
 language = {en},
 month = {April},
 pages = {1--7},
 publisher = {ACM},
 shorttitle = {{BoldMove}},
 title = {{BoldMove}: {Enabling} {IoT} {Device} {Control} on {Ubiquitous} {Touch} {Interfaces} by {Semantic} {Mapping} and {Sequential} {Selection}},
 url = {https://dl.acm.org/doi/10.1145/3491101.3519805},
 urldate = {2022-07-10},
 year = {2022}
}

@inproceedings{zhang_framework_2023,
 address = {Cancun, Quintana Roo Mexico},
 author = {Zhang, Han and Wang, Leijie and Sheng, Yilun and Xu, Xuhai and Mankoff, Jennifer and Dey, Anind K},
 booktitle = {Adjunct {Proceedings} of the 2023 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing} \& the 2023 {ACM} {International} {Symposium} on {Wearable} {Computing}},
 doi = {10.1145/3594739.3610677},
 file = {Full Text:/Users/orsonxu/Zotero/storage/W6689ZIW/Zhang et al. - 2023 - A Framework for Designing Fair Ubiquitous Computing Systems.pdf:application/pdf},
 isbn = {9798400702006},
 language = {en},
 month = {October},
 pages = {366--373},
 publisher = {ACM},
 title = {A {Framework} for {Designing} {Fair} {Ubiquitous} {Computing} {Systems}},
 url = {https://dl.acm.org/doi/10.1145/3594739.3610677},
 urldate = {2024-09-23},
 year = {2023}
}

@article{zhang_impact_2022,
 abstract = {The COVID-19 pandemic upended college education and the experiences of students due to the rapid and uneven shift to online learning. This study examined the experiences of students with disabilities with online learning, with a consideration of surrounding stressors such as financial pressures. In a mixed method approach, we compared 28 undergraduate students with disabilities (including mental health concerns) to their peers during 2020, to assess differences and similarities in their educational concerns, stress levels and COVID-19 related adversities. We found that students with disabilities entered the Spring quarter of 2020 with significantly higher concerns about classes going online, and reported more recent negative life events than other students. These differences between the two groups diminished three months later with the exception of recent negative life events. For a fuller understanding of students’ experiences, we conducted qualitative analysis of open ended interviews. We examined both positive and negative experiences with online learning among students with disabilities and mental health concerns. We describe how online learning enabled greater access–e.g., reducing the need for travel to campus–alongside ways in which online learning impeded academic engagement–e.g., reducing interpersonal interaction. We highlight a need for learning systems to meet the diverse and dynamic needs of students with disabilities.},
 author = {Zhang, Han and Morris, Margaret E and Nurius, Paula S and Mack, Kelly Avery and Brown, Jennifer and Kuehn, Kevin S and Sefidgar, Yasaman and Xu, Xuhai and Riskin, Eve A and Dey, Anind K and Mankoff, Jennifer},
 doi = {10.1145/3538514},
 file = {Zhang et al. - 2022 - Impact of Online Learning in the Context of COVID-.pdf:/Users/orsonxu/Zotero/storage/KSW3T3G8/Zhang et al. - 2022 - Impact of Online Learning in the Context of COVID-.pdf:application/pdf},
 issn = {1936-7228, 1936-7236},
 journal = {ACM Transactions on Accessible Computing},
 language = {en},
 month = {July},
 pages = {3538514},
 title = {Impact of {Online} {Learning} in the {Context} of {COVID}-19 on {Undergraduates} with {Disabilities} and {Mental} {Health} {Concerns}},
 url = {https://dl.acm.org/doi/10.1145/3538514},
 urldate = {2022-10-01},
 year = {2022}
}

@inproceedings{zhang_rethinking_2024,
 address = {Honolulu HI USA},
 author = {Zhang, Shao and Yu, Jianing and Xu, Xuhai and Yin, Changchang and Lu, Yuxuan and Yao, Bingsheng and Tory, Melanie and Padilla, Lace M and Caterino, Jeffrey and Zhang, Ping and Wang, Dakuo},
 booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3613904.3642343},
 file = {Full Text PDF:/Users/orsonxu/Zotero/storage/DWXSK4JH/Zhang et al. - 2024 - Rethinking Human-AI Collaboration in Complex Medical Decision Making A Case Study in Sepsis Diagnos.pdf:application/pdf},
 isbn = {9798400703300},
 language = {en},
 month = {May},
 pages = {1--18},
 publisher = {ACM},
 shorttitle = {Rethinking {Human}-{AI} {Collaboration} in {Complex} {Medical} {Decision} {Making}},
 title = {Rethinking {Human}-{AI} {Collaboration} in {Complex} {Medical} {Decision} {Making}: {A} {Case} {Study} in {Sepsis} {Diagnosis}},
 url = {https://dl.acm.org/doi/10.1145/3613904.3642343},
 urldate = {2024-05-23},
 year = {2024}
}

@inproceedings{zhang_voicemoji_2021,
 address = {Yokohama Japan},
 author = {Zhang, Mingrui Ray and Wang, Ruolin and Xu, Xuhai and Li, Qisheng and Sharif, Ather and Wobbrock, Jacob O},
 booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
 doi = {10.1145/3411764.3445338},
 file = {Zhang et al. - 2021 - Voicemoji Emoji Entry Using Voice for Visually Im.pdf:/Users/orsonxu/Zotero/storage/WQHDQZ76/Zhang et al. - 2021 - Voicemoji Emoji Entry Using Voice for Visually Im.pdf:application/pdf},
 isbn = {978-1-4503-8096-6},
 language = {en},
 month = {May},
 pages = {1--18},
 publisher = {ACM},
 shorttitle = {Voicemoji},
 title = {Voicemoji: {Emoji} {Entry} {Using} {Voice} for {Visually} {Impaired} {People}},
 url = {https://dl.acm.org/doi/10.1145/3411764.3445338},
 urldate = {2021-10-07},
 year = {2021}
}

@inproceedings{zhuang_reflectrack_2021,
 address = {Virtual Event USA},
 author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun},
 booktitle = {Proceedings of {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
 doi = {10.1145/3472749.3474805},
 file = {Zhuang et al. - 2021 - ReflecTrack Enabling 3D Acoustic Position Trackin.pdf:/Users/orsonxu/Zotero/storage/QHM9E2KR/Zhuang et al. - 2021 - ReflecTrack Enabling 3D Acoustic Position Trackin.pdf:application/pdf},
 isbn = {978-1-4503-8635-7},
 language = {en},
 month = {October},
 pages = {1050--1062},
 publisher = {ACM},
 shorttitle = {{ReflecTrack}},
 title = {{ReflecTrack}: {Enabling} {3D} {Acoustic} {Position} {Tracking} {Using} {Commodity} {Dual}-{Microphone} {Smartphones}},
 url = {https://dl.acm.org/doi/10.1145/3472749.3474805},
 urldate = {2022-11-16},
 year = {2021}
}
